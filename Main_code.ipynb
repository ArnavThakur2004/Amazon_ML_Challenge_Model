{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageFile, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random  # For random selection of units\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# List of possible units to assign to predictions\n",
    "UNITS = [\"kilogram\", \"pound\", \"watt\", \"kilovolt\", \"inch\", \"meter\", \"yard\", \"foot\", \"ton\", \"centimeter\"]\n",
    "\n",
    "# Step 1: Download Image with a Counter for 1000 images\n",
    "def download_image(image_url, counter, max_images=1000):\n",
    "    if counter[0] >= max_images:\n",
    "        return None  # Stop downloading if the limit is reached\n",
    "\n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=10)  # Set a timeout to avoid hanging requests\n",
    "        counter[0] += 1  # Increment the counter for each image (valid or invalid)\n",
    "        \n",
    "        if response.status_code == 200 and len(response.content) > 0:  # Ensure content size is valid\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            return img\n",
    "        else:\n",
    "            print(f\"Failed to download image from {image_url}, status code: {response.status_code}, content size: {len(response.content)}\")\n",
    "            return None\n",
    "    except (requests.exceptions.RequestException, UnidentifiedImageError, OSError) as e:\n",
    "        print(f\"Error downloading or opening image from {image_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 2: Preprocess the Image (Handle Grayscale to RGB conversion)\n",
    "def preprocess_image(image, image_size=(224, 224)):\n",
    "    # Check if the image is in grayscale (1 channel) and convert to RGB (3 channels)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    return preprocess(image).unsqueeze(0)\n",
    "\n",
    "# Step 3: Extract Image Features\n",
    "def extract_image_features(img_tensor, model):\n",
    "    \"\"\"\n",
    "    Pass the preprocessed image tensor through the model (feature extractor)\n",
    "    to get image features.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        features = model(img_tensor)\n",
    "    return features.squeeze().numpy()  # Convert to numpy array\n",
    "\n",
    "# Step 4: Prepare the Data for Training with a 1000 Image Limit\n",
    "def prepare_training_data(train_df, model, image_size=(224, 224), max_images=1000):\n",
    "    X, y = [], []\n",
    "    counter = [0]  # A mutable counter to track the number of images processed\n",
    "\n",
    "    for idx, row in train_df.iterrows():\n",
    "        if counter[0] >= max_images:\n",
    "            break  # Stop processing after 1000 images\n",
    "        \n",
    "        try:\n",
    "            # Only process rows where entity_value exists and is in a valid format\n",
    "            if pd.notna(row['entity_value']) and isinstance(row['entity_value'], str):\n",
    "                entity_value = row['entity_value'].split()[0]  # Extract the numeric value part\n",
    "                entity_value = float(entity_value)  # Convert to float\n",
    "\n",
    "                # Download and preprocess the image\n",
    "                img = download_image(row['image_link'], counter, max_images)\n",
    "                if img is None:\n",
    "                    continue  # Skip if image could not be downloaded or opened\n",
    "\n",
    "                img_tensor = preprocess_image(img, image_size)\n",
    "\n",
    "                # Extract image features\n",
    "                features = extract_image_features(img_tensor, model)\n",
    "\n",
    "                X.append(features)\n",
    "                y.append(entity_value)\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f\"Skipping row {idx} due to error: {e}\")\n",
    "            continue  # Skip rows with invalid entity_value or image errors\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Step 5: Train the Model\n",
    "def train_model(X_train, y_train):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Step 6: Make Predictions on Test Data with 1000 Image Limit\n",
    "def make_predictions(test_df, model, feature_extractor, image_size=(224, 224), max_images=1000):\n",
    "    predictions = []\n",
    "    counter = [0]  # A mutable counter to track the number of images processed\n",
    "\n",
    "    for idx, row in test_df.iterrows():\n",
    "        if counter[0] >= max_images:\n",
    "            predictions.append('')  # After 1000 predictions, append empty values\n",
    "            continue\n",
    "\n",
    "        img = download_image(row['image_link'], counter, max_images)\n",
    "        if img is None:\n",
    "            predictions.append('')  # Append empty value for failed downloads\n",
    "            continue\n",
    "\n",
    "        img_tensor = preprocess_image(img, image_size)\n",
    "        features = extract_image_features(img_tensor, feature_extractor)\n",
    "        prediction = model.predict([features])[0]\n",
    "\n",
    "        # Randomly select a unit and format the prediction\n",
    "        unit = random.choice(UNITS)\n",
    "        predictions.append(f\"{prediction:.2f} {unit}\")\n",
    "\n",
    "    # Append empty predictions for any remaining rows after the 1000 limit\n",
    "    while len(predictions) < len(test_df):\n",
    "        predictions.append('')\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Step 7: Save Output in the Required Format\n",
    "def save_predictions(test_df, predictions, output_file='test_out.csv'):\n",
    "    # Convert predictions to string type to avoid any issues with numeric values\n",
    "    predictions = [str(p).strip() if p != '' else '' for p in predictions]\n",
    "    \n",
    "    # Ensure that the predictions list has the same length as the test dataframe\n",
    "    output_df = pd.DataFrame({'index': test_df['index'], 'prediction': predictions})\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# Main Pipeline\n",
    "if __name__ == '__main__':\n",
    "    # Load training and test data\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "\n",
    "    # Load a pre-trained model (ResNet50 for feature extraction)\n",
    "    feature_extractor = models.resnet50(pretrained=True)\n",
    "    feature_extractor = torch.nn.Sequential(*(list(feature_extractor.children())[:-1]))  # Remove final classification layer\n",
    "\n",
    "    # Prepare training data (limit to 1000 images)\n",
    "    X, y = prepare_training_data(train_df, feature_extractor)\n",
    "\n",
    "    # Train-test split for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train a RandomForest model\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    # Validate the model (optional)\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(f\"Validation MSE: {mean_squared_error(y_val, y_pred)}\")\n",
    "\n",
    "    # Make predictions on test data (limit to 1000 images)\n",
    "    predictions = make_predictions(test_df, model, feature_extractor)\n",
    "\n",
    "    # Save the predictions\n",
    "    save_predictions(test_df, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7647bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
